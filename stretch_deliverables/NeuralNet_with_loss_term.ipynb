{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIv3mmNdmEoY",
        "outputId": "28e3589d-fbdb-4d38-bc63-f500b85b4d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairlearn in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: narwhals>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.13.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy<1.16.0,>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (1.15.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.1->fairlearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fairlearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference"
      ],
      "metadata": {
        "id": "2aYmApeUnx17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FairLoss(nn.Module):\n",
        "  \"\"\"A custom loss function that combines Binary Cross-Entropy (BCE) with fairness penalty terms.\n",
        "\n",
        "  The loss penalizes both demographic parity violations and equalized odds violations.\n",
        "  Loss = BCE + λDP × DP_penalty + λEO × EO_penalty\n",
        "\n",
        "  Args:\n",
        "      lambda_dp (float): Weight for the demographic parity penalty. Defaults to 2.0.\n",
        "      lambda_eo (float): Weight for the equalized odds penalty. Defaults to 2.0.\n",
        "  \"\"\"\n",
        "  def __init__(self, lambda_dp=2.0, lambda_eo=2.0):\n",
        "    super().__init__()\n",
        "    self.lambda_dp = lambda_dp\n",
        "    self.lambda_eo = lambda_eo\n",
        "    self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  def forward(self, logits, y_true, race):\n",
        "    \"\"\"Calculates the total loss including BCE, demographic parity, and equalized odds penalties.\n",
        "\n",
        "    Args:\n",
        "        logits (torch.Tensor): Raw model outputs (before sigmoid).\n",
        "        y_true (torch.Tensor): True binary labels.\n",
        "        race (torch.Tensor): Sensitive attribute (e.g., race) with binary values (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing total_loss, bce_loss, dp_loss, and eo_loss.\n",
        "\n",
        "    DP_penalty measures the squared difference in mean predicted probabilities between racial groups.\n",
        "    EO_penalty averages the squared differences in True Positive Rates (TPR) and False Positive Rates (FPR)\n",
        "    between racial groups for positive and negative classes separately.\n",
        "    \"\"\"\n",
        "    logits = logits.squeeze()\n",
        "    y_true = y_true.float()\n",
        "    race = race.float()\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "    bce_loss = self.bce(logits, y_true)\n",
        "\n",
        "    mask_0 = (race == 0)\n",
        "    mask_1 = (race == 1)\n",
        "\n",
        "    if mask_0.sum() > 0 and mask_1.sum() > 0:\n",
        "      dp_loss = (probs[mask_0].mean() - probs[mask_1].mean()) ** 2\n",
        "    else:\n",
        "      dp_loss = torch.tensor(0.0)\n",
        "\n",
        "    mask_pos = (y_true == 1)\n",
        "    mask_neg = (y_true == 0)\n",
        "\n",
        "    mask_0_pos = mask_0 & mask_pos\n",
        "    mask_1_pos = mask_1 & mask_pos\n",
        "    mask_0_neg = mask_0 & mask_neg\n",
        "    mask_1_neg = mask_1 & mask_neg\n",
        "\n",
        "    if mask_0_pos.sum() > 0 and mask_1_pos.sum() > 0:\n",
        "      tpr_loss = (probs[mask_0_pos].mean() - probs[mask_1_pos].mean()) ** 2\n",
        "    else:\n",
        "      tpr_loss = torch.tensor(0.0)\n",
        "\n",
        "    if mask_0_neg.sum() > 0 and mask_1_neg.sum() > 0:\n",
        "      fpr_loss = (probs[mask_0_neg].mean() - probs[mask_1_neg].mean()) ** 2\n",
        "    else:\n",
        "      fpr_loss = torch.tensor(0.0)\n",
        "\n",
        "    eo_loss = (tpr_loss + fpr_loss) / 2\n",
        "    total_loss = bce_loss + self.lambda_dp * dp_loss + self.lambda_eo * eo_loss\n",
        "\n",
        "    return total_loss, bce_loss.item(), dp_loss.item(), eo_loss.item()"
      ],
      "metadata": {
        "id": "W0J3VnWhnyPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    \"\"\"A 4-layer feedforward neural network.\n",
        "\n",
        "    The network consists of an input layer (2 features), two hidden layers with ReLU activation\n",
        "    and dropout, and an output layer for binary classification.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.net = nn.Sequential(\n",
        "        nn.Linear(2, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(32, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16, 1)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"Performs a forward pass through the neural network.\n",
        "\n",
        "      Args:\n",
        "          x (torch.Tensor): Input features.\n",
        "\n",
        "      Returns:\n",
        "          torch.Tensor: Raw output logits from the network.\n",
        "      \"\"\"\n",
        "      return self.net(x)"
      ],
      "metadata": {
        "id": "AiDwWyoFqUGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/lsac_data.csv\")\n",
        "if \"ZFYGPA\" in df.columns:\n",
        "    df = df.rename(columns={\"ZFYGPA\": \"zfygpa\"})\n",
        "\n",
        "needed = [\"race\", \"gender\", \"lsat\", \"ugpa\", \"zfygpa\"]\n",
        "\n",
        "for c in needed:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Label: above median zfygpa\n",
        "cutoff_value = np.median(df[\"zfygpa\"])\n",
        "df[\"admit_sim\"] = (df[\"zfygpa\"] >= cutoff_value).astype(int)\n",
        "\n",
        "# Features (drop label, zfygpa, protected attrs)\n",
        "X = df.select_dtypes(include=\"number\").drop(\n",
        "    columns=[\"zfygpa\", \"admit_sim\", \"race\", \"gender\"], errors=\"ignore\"\n",
        ")\n",
        "y = df[\"admit_sim\"].astype(int)\n",
        "gender = df[\"gender\"].astype(int)\n",
        "race = df[\"race\"].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test, g_train, g_test, r_train, r_test = train_test_split(\n",
        "    X, y, gender, race, stratify=y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "X_train = torch.FloatTensor(X_train.to_numpy())\n",
        "X_test = torch.FloatTensor(X_test.to_numpy())\n",
        "y_train = torch.FloatTensor(y_train.to_numpy())\n",
        "y_test = torch.FloatTensor(y_test.to_numpy())\n",
        "r_train = torch.LongTensor(r_train.to_numpy())\n",
        "r_test = torch.LongTensor(r_test.to_numpy())\n",
        "\n",
        "model = SimpleNN()"
      ],
      "metadata": {
        "id": "Mm0ymOBNqs3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = FairLoss(lambda_dp=0.05, lambda_eo=0.05)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "batch_size = 256\n",
        "epochs = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    indices = torch.randperm(len(X_train))\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        idx = indices[i:i+batch_size]\n",
        "        X_batch = X_train[idx]\n",
        "        y_batch = y_train[idx]\n",
        "        r_batch = r_train[idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss, bce, dp, eo = criterion(logits, y_batch, r_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}: BCE={bce:.4f}, DP={dp:.4f}, EO={eo:.4f}, Total={loss.item():.4f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits_test = model(X_test)\n",
        "    probs_test = torch.sigmoid(logits_test).squeeze().numpy()\n",
        "    preds_test = (probs_test > 0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_test.numpy(), preds_test)\n",
        "auc = roc_auc_score(y_test.numpy(), probs_test)\n",
        "dp = demographic_parity_difference(y_test.numpy(), preds_test, sensitive_features=r_test.numpy())\n",
        "eo = equalized_odds_difference(y_test.numpy(), preds_test, sensitive_features=r_test.numpy())\n",
        "\n",
        "print(f\"\\nNeural Network (Original Labels):\")\n",
        "print(f\"Accuracy: {acc:.3f} | AUC: {auc:.3f}\")\n",
        "print(f\"DP difference: {dp:.3f} | EO difference: {eo:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch0VmSDfrc3I",
        "outputId": "be9d70a7-7d44-4eb0-8736-30fe7b9eabea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: BCE=0.6652, DP=0.0140, EO=0.0178, Total=0.6668\n",
            "Epoch 40: BCE=0.6800, DP=0.0294, EO=0.0196, Total=0.6824\n",
            "Epoch 60: BCE=0.6434, DP=0.0813, EO=0.0345, Total=0.6492\n",
            "Epoch 80: BCE=0.6677, DP=0.0185, EO=0.0189, Total=0.6696\n",
            "Epoch 100: BCE=0.6683, DP=0.0431, EO=0.0425, Total=0.6725\n",
            "Epoch 120: BCE=0.6347, DP=0.0760, EO=0.0557, Total=0.6412\n",
            "Epoch 140: BCE=0.6925, DP=0.0425, EO=0.0521, Total=0.6972\n",
            "Epoch 160: BCE=0.6594, DP=0.0728, EO=0.0328, Total=0.6647\n",
            "Epoch 180: BCE=0.6435, DP=0.0598, EO=0.0231, Total=0.6476\n",
            "Epoch 200: BCE=0.6530, DP=0.0492, EO=0.0182, Total=0.6564\n",
            "Epoch 220: BCE=0.6520, DP=0.0171, EO=0.0052, Total=0.6531\n",
            "Epoch 240: BCE=0.6856, DP=0.0668, EO=0.0575, Total=0.6919\n",
            "Epoch 260: BCE=0.6451, DP=0.0284, EO=0.0103, Total=0.6471\n",
            "Epoch 280: BCE=0.6449, DP=0.0550, EO=0.0233, Total=0.6488\n",
            "Epoch 300: BCE=0.6545, DP=0.0689, EO=0.0613, Total=0.6610\n",
            "Epoch 320: BCE=0.6751, DP=0.0353, EO=0.0275, Total=0.6783\n",
            "Epoch 340: BCE=0.7123, DP=0.0527, EO=0.0811, Total=0.7190\n",
            "Epoch 360: BCE=0.6287, DP=0.0573, EO=0.0222, Total=0.6327\n",
            "Epoch 380: BCE=0.6585, DP=0.0352, EO=0.0219, Total=0.6614\n",
            "Epoch 400: BCE=0.6879, DP=0.0819, EO=0.0403, Total=0.6940\n",
            "Epoch 420: BCE=0.6601, DP=0.0430, EO=0.0757, Total=0.6660\n",
            "Epoch 440: BCE=0.6466, DP=0.0894, EO=0.0578, Total=0.6540\n",
            "Epoch 460: BCE=0.6944, DP=0.0701, EO=0.0551, Total=0.7007\n",
            "Epoch 480: BCE=0.6896, DP=0.0364, EO=0.0287, Total=0.6929\n",
            "Epoch 500: BCE=0.6302, DP=0.0636, EO=0.0240, Total=0.6346\n",
            "\n",
            "Neural Network (Original Labels):\n",
            "Accuracy: 0.590 | AUC: 0.631\n",
            "DP difference: 0.473 | EO difference: 0.503\n"
          ]
        }
      ]
    }
  ]
}