# -*- coding: utf-8 -*-
"""CatBoostPostProcess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ciu6b3Fq3gBA3ryjKfapg7ecOMKhFHXf
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, roc_auc_score
from catboost import CatBoostClassifier
from fairlearn.postprocessing import ThresholdOptimizer
from fairlearn.metrics import (
    MetricFrame,
    selection_rate,
    demographic_parity_difference,
    equalized_odds_difference,
)
from visualization_utils import save_results_visualization, log_summary_row

df = pd.read_csv("/content/lsac_data.csv")
if "ZFYGPA" in df.columns:
    df = df.rename(columns={"ZFYGPA": "zfygpa"})

needed = ["race", "gender", "lsat", "ugpa", "zfygpa"]

for c in needed:
    df[c] = pd.to_numeric(df[c], errors="coerce")

# Label: above median zfygpa
cutoff_value = np.median(df["zfygpa"])
df["admit_sim"] = (df["zfygpa"] >= cutoff_value).astype(int)

# Features (drop label, zfygpa, protected attrs)
X = df.select_dtypes(include="number").drop(
    columns=["zfygpa", "admit_sim", "race", "gender"], errors="ignore"
)
y = df["admit_sim"].astype(int)
gender = df["gender"].astype(int)
race = df["race"].astype(int)

X_train, X_test, y_train, y_test, g_train, g_test, r_train, r_test = train_test_split(
    X, y, gender, race, stratify=y, test_size=0.25, random_state=42
)

catboost = CatBoostClassifier(
    iterations=200,
    depth=5,
    learning_rate=0.05,
    subsample=0.9,
    l2_leaf_reg=1.0,
    loss_function="Logloss",
    random_seed=42,
    verbose=False
)

catboost.fit(X_train, y_train)

eo_optimizer = ThresholdOptimizer(
    estimator=catboost,
    constraints="equalized_odds",
    predict_method="predict_proba"
)

eo_optimizer.fit(X_train, y_train, sensitive_features=r_train)

# Make post-processed predictions
pred_post = eo_optimizer.predict(X_test, sensitive_features=r_test)
proba_post = catboost.predict_proba(X_test)[:, 1]
overall_acc = accuracy_score(y_test, pred_post)
overall_auc = roc_auc_score(y_test, proba_post)
print(f"Baseline CatBoost | Accuracy: {overall_acc:.3f} | AUC: {overall_auc:.3f}")

def fairness(sens, name):
    mf = MetricFrame(
        metrics={"accuracy": accuracy_score, "selection_rate": selection_rate},
        y_true=y_test,
        y_pred=pred_post,
        sensitive_features=sens,
    )

    dp = demographic_parity_difference(y_test, pred_post, sensitive_features=sens)
    eo = equalized_odds_difference(y_test, pred_post, sensitive_features=sens)

    print(f"\n=== {name.upper()} ===")
    print("Selection rates:\n", mf.by_group["selection_rate"])
    print("Accuracies:\n", mf.by_group["accuracy"])
    print(f"DP difference: {dp:.3f}")
    print(f"EO difference: {eo:.3f}")
    return dp, eo


dp_gender, eo_gender = fairness(g_test, "gender")
dp_race, eo_race = fairness(r_test, "race")

save_results_visualization('catboost_postprocess', y_test, pred_post, proba_post, sensitive_features_dict={"gender": g_test, "race": r_test})

log_summary_row('catboost_postprocess', overall_acc, overall_auc, dp_gender, eo_gender, dp_race, eo_race, '/content/model_results.csv')